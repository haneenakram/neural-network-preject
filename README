# Fine-Grained Fruit Quality Assessment using Deep Learning

## Team CHP_5

- **Haneen Akram Ahmed**
- **Menna Ali Thabet**
- **Mohamed Ashraf Fathy**
- **Reem Ahmed Ismail**
- **Zeina Shawkat**
- **Islam Hesham**

## üìã Project Overview

This project implements multiple deep learning architectures for fine-grained fruit quality assessment, focusing on classifying bananas and tomatoes into specific ripeness and quality categories. The models are designed to automatically assess and categorize fruit quality based on visual characteristics, addressing the challenge of class imbalance in agricultural datasets.

### üéØ Objective

Develop robust deep learning models capable of performing fine-grained classification of fruit quality using image features, specifically targeting:

- **Banana categories**: overripe, ripe, rotten, unripe
- **Tomato categories**: fully ripened, green, half ripened

## üìä Dataset

The dataset contains **7,395 images** across 7 quality categories with significant class imbalance:

| Category             | Count | Percentage |
| -------------------- | ----- | ---------- |
| banana_rotten        | 1,987 | 26.9%      |
| banana_ripe          | 1,440 | 19.5%      |
| banana_overripe      | 1,395 | 18.9%      |
| banana_unripe        | 1,370 | 18.5%      |
| tomato_green         | 334   | 4.5%       |
| tomato_half_ripened  | 81    | 1.1%       |
| tomato_fully_ripened | 50    | 0.7%       |

**Imbalance Ratio**: 39.74 (largest/smallest class)

## üèóÔ∏è Model Architectures

We implemented and compared **6 different architectures** to solve this fine-grained classification problem:

### 1. Vision Transformer (ViT) - **Best Performance** üèÜ

- **Custom ViT architecture** designed specifically for fruit quality assessment
- **Validation Accuracy**: 95.39%
- **Parameters**: 7.77M (29.65 MB)

**Key Components**:

- Patch Creation: 224√ó224√ó3 ‚Üí 784 patches (8√ó8)
- Embedding: 128-dimensional patch embeddings + positional encoding
- Transformer: 8 layers, 8 attention heads, GELU activation
- Classification Head: Global average pooling + MLP (2048‚Üí1024‚Üí7)

### 2. CNN + Transformer Hybrid

- **Validation Accuracy**: 96.00%
- **Parameters**: ~7.5M
- Combines CNN feature extraction with transformer attention mechanism

### 3. CoAtNet (Convolutional + Attention)

- **Validation Accuracy**: 95.45%
- Hybrid architecture combining:
  - Convolutional stem for local features
  - MBConv blocks for efficient representation
  - Transformer blocks for global reasoning

### 4. Baseline CNN

- **Validation Accuracy**: 92.2%
- Three convolutional blocks (32, 64, 128 filters)
- ReLU activations, max pooling, dropout regularization

### 5. ResNet-34 with Auxiliary Classifier

- **Validation Accuracy**: ~95%
- Standard ResNet34 with additional auxiliary output
- Cosine learning rate decay with Adam optimizer

### 6. Custom ResNet-50

- **Validation Accuracy**: 93-94%
- Modified ResNet50 with enhanced regularization
- L2 regularization, dropout, batch normalization

## üîß Training Configuration

### Data Preprocessing & Augmentation

- **Image Size**: 224√ó224√ó3
- **Normalization**: Pixel values scaled to [0,1]
- **Augmentation Techniques**:
  - Rotation (¬±20-40¬∞)
  - Zoom (¬±10-20%)
  - Width/height shifts (¬±10-20%)
  - Shear transformation (10-20%)
  - Horizontal flipping
  - Brightness/contrast adjustment

### Training Strategy

- **Batch Size**: 32
- **Epochs**: 50-60 (with early stopping)
- **Optimizer**: AdamW with weight decay (1e-5)
- **Learning Rate**: 1e-4 with reduction on plateau
- **Loss Function**: Sparse Categorical Cross-Entropy
- **Class Weighting**: Dynamic weights to handle imbalance (0.48 to 19.02)

## üìà Results Summary

| Model              | Validation Accuracy | Training Accuracy | Parameters |
| ------------------ | ------------------- | ----------------- | ---------- |
| Vision Transformer | 95.39%              | 96.25%            | 7.77M      |
| CNN + Transformer  | 96.00%              | 97.00%            | 7.5M       |
| CoAtNet            | 95.45%              | 96.67%            | -          |
| ResNet-34          | ~95%                | 92.76%            | -          |
| Custom ResNet-50   | 93-94%              | 97.42%            | -          |
| Baseline CNN       | 92.2%               | 94.2%             | -          |

### Key Findings

- **Custom architectures outperformed pre-trained models** for this specific task
- **Class weighting proved more effective** than synthetic oversampling
- **Transformer-based models showed superior performance** in handling fine-grained distinctions
- **Consistent performance** across both banana and tomato categories despite severe imbalance

## üõ†Ô∏è Implementation Details

### Technologies Used

- **Framework**: TensorFlow/Keras
- **Environment**: Kaggle Notebooks
- **Libraries**: NumPy, Pandas, Matplotlib, scikit-learn
- **Hardware**: GPU acceleration for training

### Training Techniques

- **Early Stopping**: Patience of 10 epochs
- **Model Checkpointing**: Save best weights
- **Learning Rate Scheduling**: Reduction on plateau
- **Mixed Precision Training**: For computational efficiency
- **Stratified Validation Split**: 80/20 train/validation

## üìÅ Repository Structure

```
fruit-quality-assessment/
‚îú‚îÄ‚îÄ CNN/
‚îÇ   ‚îú‚îÄ‚îÄ CNN_Report.pdf
‚îÇ   ‚îú‚îÄ‚îÄ cnn.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ model_architecture.py
‚îÇ   ‚îî‚îÄ‚îÄ training_logs/
‚îú‚îÄ‚îÄ CNN + Transformer Hybrid Model/
‚îÇ   ‚îú‚îÄ‚îÄ CNN+Transformer.ipynb
‚îÇ   ‚îî‚îÄ‚îÄ CNN+Transformer_Hybrid_Report.pdf
‚îú‚îÄ‚îÄ CoAtNet/
‚îÇ   ‚îú‚îÄ‚îÄ coAtNet.ipynb
‚îÇ   ‚îî‚îÄ‚îÄ CoAtNet_Report.pdf
‚îú‚îÄ‚îÄ ResNet-34/
‚îÇ   ‚îú‚îÄ‚îÄ ResNet34.ipynb
‚îÇ   ‚îî‚îÄ‚îÄ ResNet34_Report.pdf
‚îú‚îÄ‚îÄ ResNet-50/
‚îÇ   ‚îú‚îÄ‚îÄ resnet50.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ augmentation_pipeline.py
‚îÇ   ‚îî‚îÄ‚îÄ ResNet50_Report.pdf
‚îú‚îÄ‚îÄ ViT/
‚îÇ   ‚îú‚îÄ‚îÄ ViT.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ Vision_Transformer_Report.py
‚îÇ   ‚îú‚îÄ‚îÄ vit_classes.py
‚îÇ   ‚îú‚îÄ‚îÄ fruit_quality_vit_complete.keras
‚îÇ   ‚îî‚îÄ‚îÄ vit_weights.h5
‚îú‚îÄ‚îÄ .gitignore
‚îî‚îÄ‚îÄ README.md
```

## üöÄ Getting Started

### Prerequisites

```bash
pip install tensorflow>=2.8.0
pip install numpy pandas matplotlib seaborn
pip install scikit-learn
pip install opencv-python
```

### Running the Models

1. **Clone the repository**
2. **Download the dataset** from Kaggle competition
3. **Run training notebooks** in the `notebooks/` directory
4. **Load pre-trained weights** from `weights/` directory for inference

### Inference Example

```python
from models.vision_transformer import create_vit_model
import tensorflow as tf

# Load model
model = create_vit_model()
model.load_weights('weights/best_vit_model.h5')

# Predict on new image
prediction = model.predict(preprocessed_image)
class_names = ['banana_overripe', 'banana_ripe', 'banana_rotten',
               'banana_unripe', 'tomato_fully_ripened', 'tomato_green',
               'tomato_half_ripened']
predicted_class = class_names[np.argmax(prediction)]
```

## üîç Key Insights

### What Worked Best

1. **Custom Vision Transformer** architecture specifically designed for the task
2. **Class weighting** over synthetic data augmentation for imbalance handling
3. **Appropriate patch size** (8√ó8) for capturing fruit texture details
4. **Multi-stage learning rate scheduling** for optimal convergence

### Lessons Learned

- Pre-trained models don't always transfer well to specialized domains
- Fine-grained classification benefits from attention mechanisms
- Proper handling of class imbalance is crucial for fair evaluation
- Architectural choices should align with problem characteristics

## üèÜ Competition Results

- **Kaggle Competition**: Fine-Grained Fruit Quality Assessment
- **Best Submission**: Vision Transformer model with 95.39% validation accuracy
- **Submission Format**: CSV with ImageID, Class (0-6), and ClassName

## üìö References

1. Dosovitskiy, A., et al. "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale." ICLR 2021.
2. Dai, Z., et al. "CoAtNet: Marrying Convolution and Attention for All Data Sizes." NeurIPS 2021.
3. He, K., et al. "Deep Residual Learning for Image Recognition." CVPR 2016.

## üìÑ License

This project is part of an academic assignment for deep learning course. All rights reserved to the team members.

---

_Project completed as part of Deep Learning course - Spring 2025_
