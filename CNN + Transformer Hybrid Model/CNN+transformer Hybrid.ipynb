{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":100293,"databundleVersionId":12025815,"sourceType":"competition"}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ----- Imports -----\nimport os, numpy as np, pandas as pd, tensorflow as tf\nfrom tensorflow.keras import layers, models\nfrom tensorflow.keras.optimizers import AdamW\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.model_selection import train_test_split\nfrom glob import glob\n\n# ----- Config -----\nSEED = 42\nBATCH_SIZE = 32\nIMG_SIZE = 224\nEPOCHS = 60\nLR = 1e-4\nNUM_CLASSES = 7\nAUTOTUNE = tf.data.AUTOTUNE\nTRAIN_DIR = '/kaggle/input/fine-grained-fruit-quality-assessment/train/train'\nTEST_DIR = '/kaggle/input/fine-grained-fruit-quality-assessment/test/testNew'\nCLASS_NAMES = ['banana_overripe', 'banana_ripe', 'banana_rotten', 'banana_unripe',\n               'tomato_fully_ripened', 'tomato_green', 'tomato_half_ripened']\nclass_to_idx = {cls: i for i, cls in enumerate(CLASS_NAMES)}\n\n# ----- Model -----\ndef build_model(input_shape=(IMG_SIZE, IMG_SIZE, 3), num_classes=NUM_CLASSES):\n    inputs = layers.Input(shape=input_shape)\n    x = layers.Conv2D(32, 3, padding='same', activation='relu')(inputs)\n    x = layers.BatchNormalization()(x)\n    x = layers.MaxPooling2D()(x)\n    for filters in [64, 128, 256]:\n        x = layers.Conv2D(filters, 3, padding='same', activation='relu')(x)\n        x = layers.BatchNormalization()(x)\n        x = layers.MaxPooling2D()(x)\n    x = layers.Reshape((-1, x.shape[-1]))(x)\n    attention = layers.MultiHeadAttention(num_heads=4, key_dim=32)(x, x)\n    x = layers.Add()([x, attention])\n    x = layers.LayerNormalization()(x)\n    x = layers.GlobalAveragePooling1D()(x)\n    x = layers.Dense(128, activation='relu')(x)\n    x = layers.Dropout(0.3)(x)\n    outputs = layers.Dense(num_classes, activation='softmax')(x)\n    return models.Model(inputs, outputs)\n\n# ----- Data Prep -----\ndef decode_image(filename, label=None, img_size=IMG_SIZE):\n    img = tf.io.read_file(filename)\n    img = tf.image.decode_jpeg(img, channels=3)\n    img = tf.image.resize(img, [img_size, img_size])\n    img = img / 255.0\n    return (img, label) if label is not None else img\n\ndef augment(img, label):\n    img = tf.image.random_flip_left_right(img)\n    img = tf.image.random_brightness(img, 0.2)\n    img = tf.image.random_contrast(img, 0.7, 1.3)\n    return img, label\n\n# ----- Load & Split Data -----\nimage_paths, labels = [], []\nfor cls in CLASS_NAMES:\n    paths = glob(f\"{TRAIN_DIR}/{cls}/*.jpg\")\n    image_paths.extend(paths)\n    labels.extend([class_to_idx[cls]] * len(paths))\nimage_paths, labels = np.array(image_paths), np.array(labels)\n\ntrain_paths, val_paths, train_labels, val_labels = train_test_split(\n    image_paths, labels, test_size=0.2, stratify=labels, random_state=SEED)\n\ntrain_ds = tf.data.Dataset.from_tensor_slices((train_paths, train_labels))\ntrain_ds = train_ds.map(decode_image, num_parallel_calls=AUTOTUNE).map(augment, num_parallel_calls=AUTOTUNE)\ntrain_ds = train_ds.shuffle(1024).batch(BATCH_SIZE).prefetch(AUTOTUNE)\n\nval_ds = tf.data.Dataset.from_tensor_slices((val_paths, val_labels))\nval_ds = val_ds.map(decode_image, num_parallel_calls=AUTOTUNE).batch(BATCH_SIZE).prefetch(AUTOTUNE)\n\n# ----- Class Weights -----\nclass_weights = compute_class_weight('balanced', classes=np.unique(train_labels), y=train_labels)\nclass_weights_dict = dict(enumerate(class_weights))\n\n# ----- Train -----\nmodel = build_model()\nmodel.compile(optimizer=AdamW(LR), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\nhistory = model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS, class_weight=class_weights_dict)\n\n# ----- Save Weights -----\nmodel.save_weights(\"model_weights.weights.h5\")\n\n# ----- Show Final Val Accuracy -----\nval_loss, val_acc = model.evaluate(val_ds)\nprint(f\"✅ Final Validation Accuracy: {val_acc:.4f} | Loss: {val_loss:.4f}\")\n\n# ----- Test Accuracy Note -----\nprint(\"❗Test accuracy can't be shown — Kaggle hides labels for test set.\")\nprint(\"📤 Please upload submission.csv to Kaggle to view test leaderboard score.\")\n\n# ----- Predict & Save Submission -----\ntest_files = sorted(glob(f\"{TEST_DIR}/*.jpg\"))\ntest_ids = [os.path.basename(p) for p in test_files]\ntest_ds = tf.data.Dataset.from_tensor_slices(test_files)\ntest_ds = test_ds.map(lambda x: decode_image(x), num_parallel_calls=AUTOTUNE)\ntest_ds = test_ds.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n\npreds = model.predict(test_ds)\npred_labels = np.argmax(preds, axis=1)\npred_class_names = [CLASS_NAMES[i] for i in pred_labels]\n\nsubmission = pd.DataFrame({\n    \"ImageID\": test_ids,\n    \"Class\": pred_labels,\n    \"ClassName\": pred_class_names\n})\nsubmission.to_csv(\"submission.csv\", index=False)\nprint(f\"✅ submission.csv saved with {len(submission)} entries (with both Class and ClassName)\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-17T20:32:15.248283Z","iopub.execute_input":"2025-05-17T20:32:15.248917Z","iopub.status.idle":"2025-05-17T21:06:42.653003Z","shell.execute_reply.started":"2025-05-17T20:32:15.248892Z","shell.execute_reply":"2025-05-17T21:06:42.652212Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/60\n\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 190ms/step - accuracy: 0.5189 - loss: 1.3939 - val_accuracy: 0.2224 - val_loss: 4.4879\nEpoch 2/60\n\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 169ms/step - accuracy: 0.7609 - loss: 0.8394 - val_accuracy: 0.3340 - val_loss: 2.8275\nEpoch 3/60\n\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 126ms/step - accuracy: 0.8000 - loss: 0.8096 - val_accuracy: 0.7350 - val_loss: 0.6394\nEpoch 4/60\n\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 169ms/step - accuracy: 0.8214 - loss: 0.6658 - val_accuracy: 0.8391 - val_loss: 0.3928\nEpoch 5/60\n\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 170ms/step - accuracy: 0.8474 - loss: 0.6139 - val_accuracy: 0.8837 - val_loss: 0.3013\nEpoch 6/60\n\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 169ms/step - accuracy: 0.8649 - loss: 0.5625 - val_accuracy: 0.8526 - val_loss: 0.4027\nEpoch 7/60\n\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 160ms/step - accuracy: 0.8681 - loss: 0.5596 - val_accuracy: 0.8114 - val_loss: 0.4887\nEpoch 8/60\n\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 168ms/step - accuracy: 0.8893 - loss: 0.5179 - val_accuracy: 0.9087 - val_loss: 0.2464\nEpoch 9/60\n\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 168ms/step - accuracy: 0.8850 - loss: 0.5345 - val_accuracy: 0.9148 - val_loss: 0.2234\nEpoch 10/60\n\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 151ms/step - accuracy: 0.9016 - loss: 0.4744 - val_accuracy: 0.8573 - val_loss: 0.3821\nEpoch 11/60\n\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 162ms/step - accuracy: 0.8898 - loss: 0.4645 - val_accuracy: 0.9114 - val_loss: 0.2193\nEpoch 12/60\n\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 168ms/step - accuracy: 0.9060 - loss: 0.4392 - val_accuracy: 0.9121 - val_loss: 0.2243\nEpoch 13/60\n\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 168ms/step - accuracy: 0.9035 - loss: 0.4093 - val_accuracy: 0.9222 - val_loss: 0.2087\nEpoch 14/60\n\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 128ms/step - accuracy: 0.9052 - loss: 0.4679 - val_accuracy: 0.9351 - val_loss: 0.1795\nEpoch 15/60\n\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 171ms/step - accuracy: 0.9142 - loss: 0.3702 - val_accuracy: 0.8972 - val_loss: 0.2627\nEpoch 16/60\n\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 164ms/step - accuracy: 0.9101 - loss: 0.4075 - val_accuracy: 0.9283 - val_loss: 0.1795\nEpoch 17/60\n\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 168ms/step - accuracy: 0.9246 - loss: 0.3604 - val_accuracy: 0.9277 - val_loss: 0.1924\nEpoch 18/60\n\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 160ms/step - accuracy: 0.9166 - loss: 0.3815 - val_accuracy: 0.8864 - val_loss: 0.3019\nEpoch 19/60\n\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 167ms/step - accuracy: 0.9217 - loss: 0.3643 - val_accuracy: 0.9128 - val_loss: 0.2230\nEpoch 20/60\n\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 169ms/step - accuracy: 0.9181 - loss: 0.4442 - val_accuracy: 0.9135 - val_loss: 0.2197\nEpoch 21/60\n\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 145ms/step - accuracy: 0.9205 - loss: 0.3652 - val_accuracy: 0.9033 - val_loss: 0.2598\nEpoch 22/60\n\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 162ms/step - accuracy: 0.9244 - loss: 0.3430 - val_accuracy: 0.9459 - val_loss: 0.1491\nEpoch 23/60\n\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 171ms/step - accuracy: 0.9307 - loss: 0.3296 - val_accuracy: 0.9175 - val_loss: 0.2442\nEpoch 24/60\n\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 173ms/step - accuracy: 0.9299 - loss: 0.3460 - val_accuracy: 0.9256 - val_loss: 0.2004\nEpoch 25/60\n\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 130ms/step - accuracy: 0.9263 - loss: 0.2837 - val_accuracy: 0.9391 - val_loss: 0.1674\nEpoch 26/60\n\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 166ms/step - accuracy: 0.9407 - loss: 0.2894 - val_accuracy: 0.9243 - val_loss: 0.1991\nEpoch 27/60\n\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 169ms/step - accuracy: 0.9332 - loss: 0.3050 - val_accuracy: 0.9432 - val_loss: 0.1669\nEpoch 28/60\n\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 166ms/step - accuracy: 0.9425 - loss: 0.2442 - val_accuracy: 0.9506 - val_loss: 0.1466\nEpoch 29/60\n\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 151ms/step - accuracy: 0.9452 - loss: 0.2448 - val_accuracy: 0.9385 - val_loss: 0.1573\nEpoch 30/60\n\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 165ms/step - accuracy: 0.9421 - loss: 0.2666 - val_accuracy: 0.9486 - val_loss: 0.1474\nEpoch 31/60\n\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 175ms/step - accuracy: 0.9474 - loss: 0.2132 - val_accuracy: 0.9540 - val_loss: 0.1342\nEpoch 32/60\n\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 148ms/step - accuracy: 0.9404 - loss: 0.2416 - val_accuracy: 0.9567 - val_loss: 0.1325\nEpoch 33/60\n\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 172ms/step - accuracy: 0.9529 - loss: 0.1956 - val_accuracy: 0.9351 - val_loss: 0.1809\nEpoch 34/60\n\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 167ms/step - accuracy: 0.9536 - loss: 0.1871 - val_accuracy: 0.9412 - val_loss: 0.1685\nEpoch 35/60\n\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 166ms/step - accuracy: 0.9585 - loss: 0.1441 - val_accuracy: 0.9547 - val_loss: 0.1465\nEpoch 36/60\n\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 132ms/step - accuracy: 0.9578 - loss: 0.1983 - val_accuracy: 0.9459 - val_loss: 0.1628\nEpoch 37/60\n\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 167ms/step - accuracy: 0.9541 - loss: 0.1731 - val_accuracy: 0.9533 - val_loss: 0.1457\nEpoch 38/60\n\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 170ms/step - accuracy: 0.9583 - loss: 0.1366 - val_accuracy: 0.9398 - val_loss: 0.1776\nEpoch 39/60\n\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 164ms/step - accuracy: 0.9518 - loss: 0.2144 - val_accuracy: 0.9486 - val_loss: 0.1420\nEpoch 40/60\n\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 147ms/step - accuracy: 0.9659 - loss: 0.1458 - val_accuracy: 0.9087 - val_loss: 0.2578\nEpoch 41/60\n\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 167ms/step - accuracy: 0.9554 - loss: 0.1650 - val_accuracy: 0.9615 - val_loss: 0.1233\nEpoch 42/60\n\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 176ms/step - accuracy: 0.9621 - loss: 0.1376 - val_accuracy: 0.9378 - val_loss: 0.1980\nEpoch 43/60\n\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 149ms/step - accuracy: 0.9601 - loss: 0.1553 - val_accuracy: 0.9601 - val_loss: 0.1290\nEpoch 44/60\n\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 162ms/step - accuracy: 0.9661 - loss: 0.1041 - val_accuracy: 0.9459 - val_loss: 0.1656\nEpoch 45/60\n\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 177ms/step - accuracy: 0.9644 - loss: 0.1177 - val_accuracy: 0.9405 - val_loss: 0.1963\nEpoch 46/60\n\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 178ms/step - accuracy: 0.9702 - loss: 0.0836 - val_accuracy: 0.9540 - val_loss: 0.1477\nEpoch 47/60\n\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 125ms/step - accuracy: 0.9602 - loss: 0.1212 - val_accuracy: 0.9459 - val_loss: 0.1750\nEpoch 48/60\n\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 167ms/step - accuracy: 0.9692 - loss: 0.0995 - val_accuracy: 0.9446 - val_loss: 0.1732\nEpoch 49/60\n\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 176ms/step - accuracy: 0.9605 - loss: 0.1510 - val_accuracy: 0.9175 - val_loss: 0.2636\nEpoch 50/60\n\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 158ms/step - accuracy: 0.9620 - loss: 0.1399 - val_accuracy: 0.9594 - val_loss: 0.1353\nEpoch 51/60\n\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 150ms/step - accuracy: 0.9718 - loss: 0.0711 - val_accuracy: 0.9574 - val_loss: 0.1282\nEpoch 52/60\n\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 165ms/step - accuracy: 0.9750 - loss: 0.0778 - val_accuracy: 0.9310 - val_loss: 0.2200\nEpoch 53/60\n\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 169ms/step - accuracy: 0.9656 - loss: 0.1163 - val_accuracy: 0.9493 - val_loss: 0.1642\nEpoch 54/60\n\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 153ms/step - accuracy: 0.9687 - loss: 0.0858 - val_accuracy: 0.9412 - val_loss: 0.1797\nEpoch 55/60\n\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 163ms/step - accuracy: 0.9740 - loss: 0.0635 - val_accuracy: 0.9621 - val_loss: 0.1530\nEpoch 56/60\n\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 166ms/step - accuracy: 0.9794 - loss: 0.0474 - val_accuracy: 0.9439 - val_loss: 0.2042\nEpoch 57/60\n\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 165ms/step - accuracy: 0.9805 - loss: 0.0593 - val_accuracy: 0.9432 - val_loss: 0.2035\nEpoch 58/60\n\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 125ms/step - accuracy: 0.9715 - loss: 0.0630 - val_accuracy: 0.9621 - val_loss: 0.1364\nEpoch 59/60\n\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 168ms/step - accuracy: 0.9696 - loss: 0.0773 - val_accuracy: 0.9608 - val_loss: 0.1447\nEpoch 60/60\n\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 168ms/step - accuracy: 0.9824 - loss: 0.0424 - val_accuracy: 0.9588 - val_loss: 0.1426\n\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 78ms/step - accuracy: 0.9610 - loss: 0.1285\n✅ Final Validation Accuracy: 0.9588 | Loss: 0.1426\n❗Test accuracy can't be shown — Kaggle hides labels for test set.\n📤 Please upload submission.csv to Kaggle to view test leaderboard score.\n\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 75ms/step\n✅ submission.csv saved with 2484 entries (with both Class and ClassName)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# ----- Test Script to Load & Evaluate Saved Model -----\ndef test_model(weights_path=\"/kaggle/working/model_weights.weights.h5\"):\n    print(\"🔄 Building model and loading weights...\")\n    model = build_model()\n    model.compile(optimizer=AdamW(LR), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n    model.load_weights(weights_path)\n\n    print(\"📊 Evaluating loaded model on validation set...\")\n    loss, acc = model.evaluate(val_ds, verbose=0)\n    print(\"✅ Loaded model from saved weights\")\n    print(f\"📈 Validation Accuracy (from saved model): {acc:.4f}\")\n    print(f\"📉 Validation Loss: {loss:.4f}\")\n\n# ✅ Run the test\ntest_model()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T23:30:17.489235Z","iopub.execute_input":"2025-05-17T23:30:17.489980Z","iopub.status.idle":"2025-05-17T23:30:17.497476Z","shell.execute_reply.started":"2025-05-17T23:30:17.489955Z","shell.execute_reply":"2025-05-17T23:30:17.496610Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nimport tensorflow as tf\nfrom glob import glob\nfrom tensorflow.keras.preprocessing import image\n\ndef load_cnn_transformer_model(weights_path):\n    model = build_model()\n    model.compile(optimizer=AdamW(LR), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n    model.load_weights(weights_path)\n    print(\"✅ Model loaded from weights.\")\n    return model\n\ndef preprocess_image(img_path):\n    img = tf.io.read_file(img_path)\n    img = tf.image.decode_jpeg(img, channels=3)\n    img = tf.image.resize(img, [IMG_SIZE[0], IMG_SIZE[1]])\n    img = img / 255.0\n    return img\n\ndef prepare_test_dataset(test_dir):\n    file_paths = sorted(glob(f\"{test_dir}/*.jpg\"))\n    dataset = tf.data.Dataset.from_tensor_slices(file_paths)\n    dataset = dataset.map(lambda x: preprocess_image(x), num_parallel_calls=AUTOTUNE)\n    dataset = dataset.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n    return dataset, file_paths\n\ndef predict_and_save_submission(weights_path, test_dir, output_file='submission.csv'):\n    model = load_cnn_transformer_model(weights_path)\n    test_ds, file_paths = prepare_test_dataset(test_dir)\n\n    print(\"🔍 Predicting...\")\n    predictions = model.predict(test_ds)\n    pred_classes = np.argmax(predictions, axis=1)\n    pred_class_names = [CLASS_NAMES[i] for i in pred_classes]\n\n    image_ids = [os.path.basename(p) for p in file_paths]\n\n    submission = pd.DataFrame({\n        \"ImageID\": image_ids,\n        \"Class\": pred_classes,\n        \"ClassName\": pred_class_names\n    })\n\n    submission.to_csv(output_file, index=False)\n    print(f\"✅ submission saved to: {output_file}\")\n    print(submission.head())\n\n    return submission\n\n# Example usage\n# predict_and_save_submission(\"/kaggle/working/model_weights.weights.h5\", \"/kaggle/input/fine-grained-fruit-quality-assessment/test/testNew\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # ----- Imports -----\n# import os, numpy as np, pandas as pd, tensorflow as tf\n# from tensorflow.keras import layers, models\n# from tensorflow.keras.optimizers import AdamW\n# from sklearn.utils.class_weight import compute_class_weight\n# from sklearn.model_selection import train_test_split\n# from glob import glob\n\n# # ----- Config -----\n# SEED = 42\n# BATCH_SIZE = 32\n# IMG_SIZE = 224\n# EPOCHS = 60\n# LR = 1e-4\n# NUM_CLASSES = 7\n# AUTOTUNE = tf.data.AUTOTUNE\n# TRAIN_DIR = '/kaggle/input/fine-grained-fruit-quality-assessment/train/train'\n# TEST_DIR = '/kaggle/input/fine-grained-fruit-quality-assessment/test/testNew'\n# CLASS_NAMES = ['banana_overripe', 'banana_ripe', 'banana_rotten', 'banana_unripe',\n#                'tomato_fully_ripened', 'tomato_green', 'tomato_half_ripened']\n# class_to_idx = {cls: i for i, cls in enumerate(CLASS_NAMES)}\n\n# # ----- Model -----\n# def build_model(input_shape=(IMG_SIZE, IMG_SIZE, 3), num_classes=NUM_CLASSES):\n#     inputs = layers.Input(shape=input_shape)\n#     x = layers.Conv2D(32, 3, padding='same', activation='relu')(inputs)\n#     x = layers.BatchNormalization()(x)\n#     x = layers.MaxPooling2D()(x)\n#     for filters in [64, 128, 256]:\n#         x = layers.Conv2D(filters, 3, padding='same', activation='relu')(x)\n#         x = layers.BatchNormalization()(x)\n#         x = layers.MaxPooling2D()(x)\n#     x = layers.Reshape((-1, x.shape[-1]))(x)\n#     attention = layers.MultiHeadAttention(num_heads=4, key_dim=32)(x, x)\n#     x = layers.Add()([x, attention])\n#     x = layers.LayerNormalization()(x)\n#     x = layers.GlobalAveragePooling1D()(x)\n#     x = layers.Dense(128, activation='relu')(x)\n#     x = layers.Dropout(0.3)(x)\n#     outputs = layers.Dense(num_classes, activation='softmax')(x)\n#     return models.Model(inputs, outputs)\n\n# # ----- Data Prep -----\n# def decode_image(filename, label=None, img_size=IMG_SIZE):\n#     img = tf.io.read_file(filename)\n#     img = tf.image.decode_jpeg(img, channels=3)\n#     img = tf.image.resize(img, [img_size, img_size])\n#     img = img / 255.0\n#     return (img, label) if label is not None else img\n\n# def augment(img, label):\n#     img = tf.image.random_flip_left_right(img)\n#     img = tf.image.random_brightness(img, 0.2)\n#     img = tf.image.random_contrast(img, 0.7, 1.3)\n#     return img, label\n\n# # ----- Load & Split Data -----\n# image_paths, labels = [], []\n# for cls in CLASS_NAMES:\n#     paths = glob(f\"{TRAIN_DIR}/{cls}/*.jpg\")\n#     image_paths.extend(paths)\n#     labels.extend([class_to_idx[cls]] * len(paths))\n# image_paths, labels = np.array(image_paths), np.array(labels)\n\n# train_paths, val_paths, train_labels, val_labels = train_test_split(\n#     image_paths, labels, test_size=0.2, stratify=labels, random_state=SEED)\n\n# train_ds = tf.data.Dataset.from_tensor_slices((train_paths, train_labels))\n# train_ds = train_ds.map(decode_image, num_parallel_calls=AUTOTUNE).map(augment, num_parallel_calls=AUTOTUNE)\n# train_ds = train_ds.shuffle(1024).batch(BATCH_SIZE).prefetch(AUTOTUNE)\n\n# val_ds = tf.data.Dataset.from_tensor_slices((val_paths, val_labels))\n# val_ds = val_ds.map(decode_image, num_parallel_calls=AUTOTUNE).batch(BATCH_SIZE).prefetch(AUTOTUNE)\n\n# # ----- Class Weights -----\n# class_weights = compute_class_weight('balanced', classes=np.unique(train_labels), y=train_labels)\n# class_weights_dict = dict(enumerate(class_weights))\n\n# # ----- Train -----\n# model = build_model()\n# model.compile(optimizer=AdamW(LR), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n# history = model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS, class_weight=class_weights_dict)\n\n# # ----- Save Weights -----\n# model.save_weights(\"/kaggle/working/model_weights.weights.h5\")\n\n# # ----- Show Final Val Accuracy -----\n# val_loss, val_acc = model.evaluate(val_ds)\n# print(f\"✅ Final Validation Accuracy: {val_acc:.4f} | Loss: {val_loss:.4f}\")\n\n# # ----- Test Accuracy Note -----\n# print(\"❗Test accuracy can't be shown — Kaggle hides labels for test set.\")\n# print(\"📤 Please upload submission.csv to Kaggle to view test leaderboard score.\")\n\n# # ----- Predict & Save Submission (COMMENTED OUT) -----\n# # test_files = sorted(glob(f\"{TEST_DIR}/*.jpg\"))\n# # test_ids = [os.path.basename(p) for p in test_files]\n# # test_ds = tf.data.Dataset.from_tensor_slices(test_files)\n# # test_ds = test_ds.map(lambda x: decode_image(x), num_parallel_calls=AUTOTUNE)\n# # test_ds = test_ds.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n\n# # preds = model.predict(test_ds)\n# # pred_labels = np.argmax(preds, axis=1)\n# # pred_class_names = [CLASS_NAMES[i] for i in pred_labels]\n\n# # submission = pd.DataFrame({\n# #     \"ImageID\": test_ids,\n# #     \"Class\": pred_labels,\n# #     \"ClassName\": pred_class_names\n# # })\n# # submission.to_csv(\"submission.csv\", index=False)\n# # print(f\"✅ submission.csv saved with {len(submission)} entries (with both Class and ClassName)\")\n\n# # ----- Test Script to Load & Evaluate Saved Model -----\n# def test_model(weights_path=\"/kaggle/working/model_weights.weights.h5\"):\n#     print(\"🔄 Building model and loading weights...\")\n#     model = build_model()\n#     model.compile(optimizer=AdamW(LR), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n#     model.load_weights(weights_path)\n\n#     print(\"📊 Evaluating loaded model on validation set...\")\n#     loss, acc = model.evaluate(val_ds, verbose=0)\n#     print(\"✅ Loaded model from saved weights\")\n#     print(f\"📈 Validation Accuracy (from saved model): {acc:.4f}\")\n#     print(f\"📉 Validation Loss: {loss:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T23:13:44.106723Z","iopub.execute_input":"2025-05-17T23:13:44.107342Z","iopub.status.idle":"2025-05-17T23:13:44.112985Z","shell.execute_reply.started":"2025-05-17T23:13:44.107305Z","shell.execute_reply":"2025-05-17T23:13:44.112239Z"}},"outputs":[],"execution_count":8}]}