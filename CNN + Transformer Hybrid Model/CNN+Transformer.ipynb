{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-17T20:32:15.248917Z",
     "iopub.status.busy": "2025-05-17T20:32:15.248283Z",
     "iopub.status.idle": "2025-05-17T21:06:42.653003Z",
     "shell.execute_reply": "2025-05-17T21:06:42.652212Z",
     "shell.execute_reply.started": "2025-05-17T20:32:15.248892Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "\u001b[1m185/185\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 190ms/step - accuracy: 0.5189 - loss: 1.3939 - val_accuracy: 0.2224 - val_loss: 4.4879\n",
      "Epoch 2/60\n",
      "\u001b[1m185/185\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 169ms/step - accuracy: 0.7609 - loss: 0.8394 - val_accuracy: 0.3340 - val_loss: 2.8275\n",
      "Epoch 3/60\n",
      "\u001b[1m185/185\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 126ms/step - accuracy: 0.8000 - loss: 0.8096 - val_accuracy: 0.7350 - val_loss: 0.6394\n",
      "Epoch 4/60\n",
      "\u001b[1m185/185\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 169ms/step - accuracy: 0.8214 - loss: 0.6658 - val_accuracy: 0.8391 - val_loss: 0.3928\n",
      "Epoch 5/60\n",
      "\u001b[1m185/185\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 170ms/step - accuracy: 0.8474 - loss: 0.6139 - val_accuracy: 0.8837 - val_loss: 0.3013\n",
      "Epoch 6/60\n",
      "\u001b[1m185/185\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 169ms/step - accuracy: 0.8649 - loss: 0.5625 - val_accuracy: 0.8526 - val_loss: 0.4027\n",
      "Epoch 7/60\n",
      "\u001b[1m185/185\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 160ms/step - accuracy: 0.8681 - loss: 0.5596 - val_accuracy: 0.8114 - val_loss: 0.4887\n",
      "Epoch 8/60\n",
      "\u001b[1m185/185\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 168ms/step - accuracy: 0.8893 - loss: 0.5179 - val_accuracy: 0.9087 - val_loss: 0.2464\n",
      "Epoch 9/60\n",
      "\u001b[1m185/185\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 168ms/step - accuracy: 0.8850 - loss: 0.5345 - val_accuracy: 0.9148 - val_loss: 0.2234\n",
      "Epoch 10/60\n",
      "\u001b[1m185/185\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 151ms/step - accuracy: 0.9016 - loss: 0.4744 - val_accuracy: 0.8573 - val_loss: 0.3821\n",
      "Epoch 11/60\n",
      "\u001b[1m185/185\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 162ms/step - accuracy: 0.8898 - loss: 0.4645 - val_accuracy: 0.9114 - val_loss: 0.2193\n",
      "Epoch 12/60\n",
      "\u001b[1m185/185\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 168ms/step - accuracy: 0.9060 - loss: 0.4392 - val_accuracy: 0.9121 - val_loss: 0.2243\n",
      "Epoch 13/60\n",
      "\u001b[1m185/185\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 168ms/step - accuracy: 0.9035 - loss: 0.4093 - val_accuracy: 0.9222 - val_loss: 0.2087\n",
      "Epoch 14/60\n",
      "\u001b[1m185/185\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 128ms/step - accuracy: 0.9052 - loss: 0.4679 - val_accuracy: 0.9351 - val_loss: 0.1795\n",
      "Epoch 15/60\n",
      "\u001b[1m185/185\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 171ms/step - accuracy: 0.9142 - loss: 0.3702 - val_accuracy: 0.8972 - val_loss: 0.2627\n",
      "Epoch 16/60\n",
      "\u001b[1m185/185\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 164ms/step - accuracy: 0.9101 - loss: 0.4075 - val_accuracy: 0.9283 - val_loss: 0.1795\n",
      "Epoch 17/60\n",
      "\u001b[1m185/185\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 168ms/step - accuracy: 0.9246 - loss: 0.3604 - val_accuracy: 0.9277 - val_loss: 0.1924\n",
      "Epoch 18/60\n",
      "\u001b[1m185/185\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 160ms/step - accuracy: 0.9166 - loss: 0.3815 - val_accuracy: 0.8864 - val_loss: 0.3019\n",
      "Epoch 19/60\n",
      "\u001b[1m185/185\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 167ms/step - accuracy: 0.9217 - loss: 0.3643 - val_accuracy: 0.9128 - val_loss: 0.2230\n",
      "Epoch 20/60\n",
      "\u001b[1m185/185\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 169ms/step - accuracy: 0.9181 - loss: 0.4442 - val_accuracy: 0.9135 - val_loss: 0.2197\n",
      "Epoch 21/60\n",
      "\u001b[1m185/185\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 145ms/step - accuracy: 0.9205 - loss: 0.3652 - val_accuracy: 0.9033 - val_loss: 0.2598\n",
      "Epoch 22/60\n",
      "\u001b[1m185/185\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 162ms/step - accuracy: 0.9244 - loss: 0.3430 - val_accuracy: 0.9459 - val_loss: 0.1491\n",
      "Epoch 23/60\n",
      "\u001b[1m185/185\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 171ms/step - accuracy: 0.9307 - loss: 0.3296 - val_accuracy: 0.9175 - val_loss: 0.2442\n",
      "Epoch 24/60\n",
      "\u001b[1m185/185\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 173ms/step - accuracy: 0.9299 - loss: 0.3460 - val_accuracy: 0.9256 - val_loss: 0.2004\n",
      "Epoch 25/60\n",
      "\u001b[1m185/185\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 130ms/step - accuracy: 0.9263 - loss: 0.2837 - val_accuracy: 0.9391 - val_loss: 0.1674\n",
      "Epoch 26/60\n",
      "\u001b[1m185/185\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 166ms/step - accuracy: 0.9407 - loss: 0.2894 - val_accuracy: 0.9243 - val_loss: 0.1991\n",
      "Epoch 27/60\n",
      "\u001b[1m185/185\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 169ms/step - accuracy: 0.9332 - loss: 0.3050 - val_accuracy: 0.9432 - val_loss: 0.1669\n",
      "Epoch 28/60\n",
      "\u001b[1m185/185\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 166ms/step - accuracy: 0.9425 - loss: 0.2442 - val_accuracy: 0.9506 - val_loss: 0.1466\n",
      "Epoch 29/60\n",
      "\u001b[1m185/185\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 151ms/step - accuracy: 0.9452 - loss: 0.2448 - val_accuracy: 0.9385 - val_loss: 0.1573\n",
      "Epoch 30/60\n",
      "\u001b[1m185/185\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 165ms/step - accuracy: 0.9421 - loss: 0.2666 - val_accuracy: 0.9486 - val_loss: 0.1474\n",
      "Epoch 31/60\n",
      "\u001b[1m185/185\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 175ms/step - accuracy: 0.9474 - loss: 0.2132 - val_accuracy: 0.9540 - val_loss: 0.1342\n",
      "Epoch 32/60\n",
      "\u001b[1m185/185\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 148ms/step - accuracy: 0.9404 - loss: 0.2416 - val_accuracy: 0.9567 - val_loss: 0.1325\n",
      "Epoch 33/60\n",
      "\u001b[1m185/185\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 172ms/step - accuracy: 0.9529 - loss: 0.1956 - val_accuracy: 0.9351 - val_loss: 0.1809\n",
      "Epoch 34/60\n",
      "\u001b[1m185/185\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 167ms/step - accuracy: 0.9536 - loss: 0.1871 - val_accuracy: 0.9412 - val_loss: 0.1685\n",
      "Epoch 35/60\n",
      "\u001b[1m185/185\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 166ms/step - accuracy: 0.9585 - loss: 0.1441 - val_accuracy: 0.9547 - val_loss: 0.1465\n",
      "Epoch 36/60\n",
      "\u001b[1m185/185\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 132ms/step - accuracy: 0.9578 - loss: 0.1983 - val_accuracy: 0.9459 - val_loss: 0.1628\n",
      "Epoch 37/60\n",
      "\u001b[1m185/185\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 167ms/step - accuracy: 0.9541 - loss: 0.1731 - val_accuracy: 0.9533 - val_loss: 0.1457\n",
      "Epoch 38/60\n",
      "\u001b[1m185/185\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 170ms/step - accuracy: 0.9583 - loss: 0.1366 - val_accuracy: 0.9398 - val_loss: 0.1776\n",
      "Epoch 39/60\n",
      "\u001b[1m185/185\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 164ms/step - accuracy: 0.9518 - loss: 0.2144 - val_accuracy: 0.9486 - val_loss: 0.1420\n",
      "Epoch 40/60\n",
      "\u001b[1m185/185\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 147ms/step - accuracy: 0.9659 - loss: 0.1458 - val_accuracy: 0.9087 - val_loss: 0.2578\n",
      "Epoch 41/60\n",
      "\u001b[1m185/185\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 167ms/step - accuracy: 0.9554 - loss: 0.1650 - val_accuracy: 0.9615 - val_loss: 0.1233\n",
      "Epoch 42/60\n",
      "\u001b[1m185/185\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 176ms/step - accuracy: 0.9621 - loss: 0.1376 - val_accuracy: 0.9378 - val_loss: 0.1980\n",
      "Epoch 43/60\n",
      "\u001b[1m185/185\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 149ms/step - accuracy: 0.9601 - loss: 0.1553 - val_accuracy: 0.9601 - val_loss: 0.1290\n",
      "Epoch 44/60\n",
      "\u001b[1m185/185\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 162ms/step - accuracy: 0.9661 - loss: 0.1041 - val_accuracy: 0.9459 - val_loss: 0.1656\n",
      "Epoch 45/60\n",
      "\u001b[1m185/185\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 177ms/step - accuracy: 0.9644 - loss: 0.1177 - val_accuracy: 0.9405 - val_loss: 0.1963\n",
      "Epoch 46/60\n",
      "\u001b[1m185/185\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 178ms/step - accuracy: 0.9702 - loss: 0.0836 - val_accuracy: 0.9540 - val_loss: 0.1477\n",
      "Epoch 47/60\n",
      "\u001b[1m185/185\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 125ms/step - accuracy: 0.9602 - loss: 0.1212 - val_accuracy: 0.9459 - val_loss: 0.1750\n",
      "Epoch 48/60\n",
      "\u001b[1m185/185\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 167ms/step - accuracy: 0.9692 - loss: 0.0995 - val_accuracy: 0.9446 - val_loss: 0.1732\n",
      "Epoch 49/60\n",
      "\u001b[1m185/185\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 176ms/step - accuracy: 0.9605 - loss: 0.1510 - val_accuracy: 0.9175 - val_loss: 0.2636\n",
      "Epoch 50/60\n",
      "\u001b[1m185/185\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 158ms/step - accuracy: 0.9620 - loss: 0.1399 - val_accuracy: 0.9594 - val_loss: 0.1353\n",
      "Epoch 51/60\n",
      "\u001b[1m185/185\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 150ms/step - accuracy: 0.9718 - loss: 0.0711 - val_accuracy: 0.9574 - val_loss: 0.1282\n",
      "Epoch 52/60\n",
      "\u001b[1m185/185\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 165ms/step - accuracy: 0.9750 - loss: 0.0778 - val_accuracy: 0.9310 - val_loss: 0.2200\n",
      "Epoch 53/60\n",
      "\u001b[1m185/185\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 169ms/step - accuracy: 0.9656 - loss: 0.1163 - val_accuracy: 0.9493 - val_loss: 0.1642\n",
      "Epoch 54/60\n",
      "\u001b[1m185/185\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 153ms/step - accuracy: 0.9687 - loss: 0.0858 - val_accuracy: 0.9412 - val_loss: 0.1797\n",
      "Epoch 55/60\n",
      "\u001b[1m185/185\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 163ms/step - accuracy: 0.9740 - loss: 0.0635 - val_accuracy: 0.9621 - val_loss: 0.1530\n",
      "Epoch 56/60\n",
      "\u001b[1m185/185\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 166ms/step - accuracy: 0.9794 - loss: 0.0474 - val_accuracy: 0.9439 - val_loss: 0.2042\n",
      "Epoch 57/60\n",
      "\u001b[1m185/185\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 165ms/step - accuracy: 0.9805 - loss: 0.0593 - val_accuracy: 0.9432 - val_loss: 0.2035\n",
      "Epoch 58/60\n",
      "\u001b[1m185/185\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 125ms/step - accuracy: 0.9715 - loss: 0.0630 - val_accuracy: 0.9621 - val_loss: 0.1364\n",
      "Epoch 59/60\n",
      "\u001b[1m185/185\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 168ms/step - accuracy: 0.9696 - loss: 0.0773 - val_accuracy: 0.9608 - val_loss: 0.1447\n",
      "Epoch 60/60\n",
      "\u001b[1m185/185\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 168ms/step - accuracy: 0.9824 - loss: 0.0424 - val_accuracy: 0.9588 - val_loss: 0.1426\n",
      "\u001b[1m47/47\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 78ms/step - accuracy: 0.9610 - loss: 0.1285\n",
      "âœ… Final Validation Accuracy: 0.9588 | Loss: 0.1426\n",
      "â—Test accuracy can't be shown â€” Kaggle hides labels for test set.\n",
      "ğŸ“¤ Please upload submission.csv to Kaggle to view test leaderboard score.\n",
      "\u001b[1m78/78\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 75ms/step\n",
      "âœ… submission.csv saved with 2484 entries (with both Class and ClassName)\n"
     ]
    }
   ],
   "source": [
    "# ----- Imports -----\n",
    "import os, numpy as np, pandas as pd, tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.optimizers import AdamW\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "from glob import glob\n",
    "\n",
    "# ----- Config -----\n",
    "SEED = 42\n",
    "BATCH_SIZE = 32\n",
    "IMG_SIZE = 224\n",
    "EPOCHS = 60\n",
    "LR = 1e-4\n",
    "NUM_CLASSES = 7\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "TRAIN_DIR = '/kaggle/input/fine-grained-fruit-quality-assessment/train/train'\n",
    "TEST_DIR = '/kaggle/input/fine-grained-fruit-quality-assessment/test/testNew'\n",
    "CLASS_NAMES = ['banana_overripe', 'banana_ripe', 'banana_rotten', 'banana_unripe',\n",
    "            'tomato_fully_ripened', 'tomato_green', 'tomato_half_ripened']\n",
    "class_to_idx = {cls: i for i, cls in enumerate(CLASS_NAMES)}\n",
    "\n",
    "# ----- Model -----\n",
    "def build_model(input_shape=(IMG_SIZE, IMG_SIZE, 3), num_classes=NUM_CLASSES):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.Conv2D(32, 3, padding='same', activation='relu')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "    for filters in [64, 128, 256]:\n",
    "        x = layers.Conv2D(filters, 3, padding='same', activation='relu')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.MaxPooling2D()(x)\n",
    "    x = layers.Reshape((-1, x.shape[-1]))(x)\n",
    "    attention = layers.MultiHeadAttention(num_heads=4, key_dim=32)(x, x)\n",
    "    x = layers.Add()([x, attention])\n",
    "    x = layers.LayerNormalization()(x)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    return models.Model(inputs, outputs)\n",
    "\n",
    "# ----- Data Prep -----\n",
    "def decode_image(filename, label=None, img_size=IMG_SIZE):\n",
    "    img = tf.io.read_file(filename)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, [img_size, img_size])\n",
    "    img = img / 255.0\n",
    "    return (img, label) if label is not None else img\n",
    "\n",
    "def augment(img, label):\n",
    "    img = tf.image.random_flip_left_right(img)\n",
    "    img = tf.image.random_brightness(img, 0.2)\n",
    "    img = tf.image.random_contrast(img, 0.7, 1.3)\n",
    "    return img, label\n",
    "\n",
    "# ----- Load & Split Data -----\n",
    "image_paths, labels = [], []\n",
    "for cls in CLASS_NAMES:\n",
    "    paths = glob(f\"{TRAIN_DIR}/{cls}/*.jpg\")\n",
    "    image_paths.extend(paths)\n",
    "    labels.extend([class_to_idx[cls]] * len(paths))\n",
    "image_paths, labels = np.array(image_paths), np.array(labels)\n",
    "\n",
    "train_paths, val_paths, train_labels, val_labels = train_test_split(\n",
    "    image_paths, labels, test_size=0.2, stratify=labels, random_state=SEED)\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((train_paths, train_labels))\n",
    "train_ds = train_ds.map(decode_image, num_parallel_calls=AUTOTUNE).map(augment, num_parallel_calls=AUTOTUNE)\n",
    "train_ds = train_ds.shuffle(1024).batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
    "\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((val_paths, val_labels))\n",
    "val_ds = val_ds.map(decode_image, num_parallel_calls=AUTOTUNE).batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
    "\n",
    "# ----- Class Weights -----\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(train_labels), y=train_labels)\n",
    "class_weights_dict = dict(enumerate(class_weights))\n",
    "\n",
    "# ----- Train -----\n",
    "model = build_model()\n",
    "model.compile(optimizer=AdamW(LR), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS, class_weight=class_weights_dict)\n",
    "\n",
    "# ----- Save Weights -----\n",
    "model.save_weights(\"model_weights.weights.h5\")\n",
    "\n",
    "# ----- Show Final Val Accuracy -----\n",
    "val_loss, val_acc = model.evaluate(val_ds)\n",
    "print(f\"âœ… Final Validation Accuracy: {val_acc:.4f} | Loss: {val_loss:.4f}\")\n",
    "\n",
    "# ----- Test Accuracy Note -----\n",
    "print(\"â—Test accuracy can't be shown â€” Kaggle hides labels for test set.\")\n",
    "print(\"ğŸ“¤ Please upload submission.csv to Kaggle to view test leaderboard score.\")\n",
    "\n",
    "# ----- Predict & Save Submission -----\n",
    "test_files = sorted(glob(f\"{TEST_DIR}/*.jpg\"))\n",
    "test_ids = [os.path.basename(p) for p in test_files]\n",
    "test_ds = tf.data.Dataset.from_tensor_slices(test_files)\n",
    "test_ds = test_ds.map(lambda x: decode_image(x), num_parallel_calls=AUTOTUNE)\n",
    "test_ds = test_ds.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
    "\n",
    "preds = model.predict(test_ds)\n",
    "pred_labels = np.argmax(preds, axis=1)\n",
    "pred_class_names = [CLASS_NAMES[i] for i in pred_labels]\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"ImageID\": test_ids,\n",
    "    \"Class\": pred_labels,\n",
    "    \"ClassName\": pred_class_names\n",
    "})\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "print(f\"âœ… submission.csv saved with {len(submission)} entries (with both Class and ClassName)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ----- Test Script to Load & Evaluate Saved Model -----\n",
    "def test_model(weights_path=\"/kaggle/working/model_weights.weights.h5\"):\n",
    "    print(\"ğŸ”„ Building model and loading weights...\")\n",
    "    model = build_model()\n",
    "    model.compile(optimizer=AdamW(LR), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.load_weights(weights_path)\n",
    "\n",
    "    print(\"ğŸ“Š Evaluating loaded model on validation set...\")\n",
    "    loss, acc = model.evaluate(val_ds, verbose=0)\n",
    "    print(\"âœ… Loaded model from saved weights\")\n",
    "    print(f\"ğŸ“ˆ Validation Accuracy (from saved model): {acc:.4f}\")\n",
    "    print(f\"ğŸ“‰ Validation Loss: {loss:.4f}\")\n",
    "\n",
    "# âœ… Run the test\n",
    "test_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T23:13:44.107342Z",
     "iopub.status.busy": "2025-05-17T23:13:44.106723Z",
     "iopub.status.idle": "2025-05-17T23:13:44.112985Z",
     "shell.execute_reply": "2025-05-17T23:13:44.112239Z",
     "shell.execute_reply.started": "2025-05-17T23:13:44.107305Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # ----- Imports -----\n",
    "# import os, numpy as np, pandas as pd, tensorflow as tf\n",
    "# from tensorflow.keras import layers, models\n",
    "# from tensorflow.keras.optimizers import AdamW\n",
    "# from sklearn.utils.class_weight import compute_class_weight\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from glob import glob\n",
    "\n",
    "# # ----- Config -----\n",
    "# SEED = 42\n",
    "# BATCH_SIZE = 32\n",
    "# IMG_SIZE = 224\n",
    "# EPOCHS = 60\n",
    "# LR = 1e-4\n",
    "# NUM_CLASSES = 7\n",
    "# AUTOTUNE = tf.data.AUTOTUNE\n",
    "# TRAIN_DIR = '/kaggle/input/fine-grained-fruit-quality-assessment/train/train'\n",
    "# TEST_DIR = '/kaggle/input/fine-grained-fruit-quality-assessment/test/testNew'\n",
    "# CLASS_NAMES = ['banana_overripe', 'banana_ripe', 'banana_rotten', 'banana_unripe',\n",
    "#                'tomato_fully_ripened', 'tomato_green', 'tomato_half_ripened']\n",
    "# class_to_idx = {cls: i for i, cls in enumerate(CLASS_NAMES)}\n",
    "\n",
    "# # ----- Model -----\n",
    "# def build_model(input_shape=(IMG_SIZE, IMG_SIZE, 3), num_classes=NUM_CLASSES):\n",
    "#     inputs = layers.Input(shape=input_shape)\n",
    "#     x = layers.Conv2D(32, 3, padding='same', activation='relu')(inputs)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "#     x = layers.MaxPooling2D()(x)\n",
    "#     for filters in [64, 128, 256]:\n",
    "#         x = layers.Conv2D(filters, 3, padding='same', activation='relu')(x)\n",
    "#         x = layers.BatchNormalization()(x)\n",
    "#         x = layers.MaxPooling2D()(x)\n",
    "#     x = layers.Reshape((-1, x.shape[-1]))(x)\n",
    "#     attention = layers.MultiHeadAttention(num_heads=4, key_dim=32)(x, x)\n",
    "#     x = layers.Add()([x, attention])\n",
    "#     x = layers.LayerNormalization()(x)\n",
    "#     x = layers.GlobalAveragePooling1D()(x)\n",
    "#     x = layers.Dense(128, activation='relu')(x)\n",
    "#     x = layers.Dropout(0.3)(x)\n",
    "#     outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "#     return models.Model(inputs, outputs)\n",
    "\n",
    "# # ----- Data Prep -----\n",
    "# def decode_image(filename, label=None, img_size=IMG_SIZE):\n",
    "#     img = tf.io.read_file(filename)\n",
    "#     img = tf.image.decode_jpeg(img, channels=3)\n",
    "#     img = tf.image.resize(img, [img_size, img_size])\n",
    "#     img = img / 255.0\n",
    "#     return (img, label) if label is not None else img\n",
    "\n",
    "# def augment(img, label):\n",
    "#     img = tf.image.random_flip_left_right(img)\n",
    "#     img = tf.image.random_brightness(img, 0.2)\n",
    "#     img = tf.image.random_contrast(img, 0.7, 1.3)\n",
    "#     return img, label\n",
    "\n",
    "# # ----- Load & Split Data -----\n",
    "# image_paths, labels = [], []\n",
    "# for cls in CLASS_NAMES:\n",
    "#     paths = glob(f\"{TRAIN_DIR}/{cls}/*.jpg\")\n",
    "#     image_paths.extend(paths)\n",
    "#     labels.extend([class_to_idx[cls]] * len(paths))\n",
    "# image_paths, labels = np.array(image_paths), np.array(labels)\n",
    "\n",
    "# train_paths, val_paths, train_labels, val_labels = train_test_split(\n",
    "#     image_paths, labels, test_size=0.2, stratify=labels, random_state=SEED)\n",
    "\n",
    "# train_ds = tf.data.Dataset.from_tensor_slices((train_paths, train_labels))\n",
    "# train_ds = train_ds.map(decode_image, num_parallel_calls=AUTOTUNE).map(augment, num_parallel_calls=AUTOTUNE)\n",
    "# train_ds = train_ds.shuffle(1024).batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
    "\n",
    "# val_ds = tf.data.Dataset.from_tensor_slices((val_paths, val_labels))\n",
    "# val_ds = val_ds.map(decode_image, num_parallel_calls=AUTOTUNE).batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
    "\n",
    "# # ----- Class Weights -----\n",
    "# class_weights = compute_class_weight('balanced', classes=np.unique(train_labels), y=train_labels)\n",
    "# class_weights_dict = dict(enumerate(class_weights))\n",
    "\n",
    "# # ----- Train -----\n",
    "# model = build_model()\n",
    "# model.compile(optimizer=AdamW(LR), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "# history = model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS, class_weight=class_weights_dict)\n",
    "\n",
    "# # ----- Save Weights -----\n",
    "# model.save_weights(\"/kaggle/working/model_weights.weights.h5\")\n",
    "\n",
    "# # ----- Show Final Val Accuracy -----\n",
    "# val_loss, val_acc = model.evaluate(val_ds)\n",
    "# print(f\"âœ… Final Validation Accuracy: {val_acc:.4f} | Loss: {val_loss:.4f}\")\n",
    "\n",
    "# # ----- Test Accuracy Note -----\n",
    "# print(\"â—Test accuracy can't be shown â€” Kaggle hides labels for test set.\")\n",
    "# print(\"ğŸ“¤ Please upload submission.csv to Kaggle to view test leaderboard score.\")\n",
    "\n",
    "# # ----- Predict & Save Submission (COMMENTED OUT) -----\n",
    "# # test_files = sorted(glob(f\"{TEST_DIR}/*.jpg\"))\n",
    "# # test_ids = [os.path.basename(p) for p in test_files]\n",
    "# # test_ds = tf.data.Dataset.from_tensor_slices(test_files)\n",
    "# # test_ds = test_ds.map(lambda x: decode_image(x), num_parallel_calls=AUTOTUNE)\n",
    "# # test_ds = test_ds.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
    "\n",
    "# # preds = model.predict(test_ds)\n",
    "# # pred_labels = np.argmax(preds, axis=1)\n",
    "# # pred_class_names = [CLASS_NAMES[i] for i in pred_labels]\n",
    "\n",
    "# # submission = pd.DataFrame({\n",
    "# #     \"ImageID\": test_ids,\n",
    "# #     \"Class\": pred_labels,\n",
    "# #     \"ClassName\": pred_class_names\n",
    "# # })\n",
    "# # submission.to_csv(\"submission.csv\", index=False)\n",
    "# # print(f\"âœ… submission.csv saved with {len(submission)} entries (with both Class and ClassName)\")\n",
    "\n",
    "# # ----- Test Script to Load & Evaluate Saved Model -----\n",
    "# def test_model(weights_path=\"/kaggle/working/model_weights.weights.h5\"):\n",
    "#     print(\"ğŸ”„ Building model and loading weights...\")\n",
    "#     model = build_model()\n",
    "#     model.compile(optimizer=AdamW(LR), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "#     model.load_weights(weights_path)\n",
    "\n",
    "#     print(\"ğŸ“Š Evaluating loaded model on validation set...\")\n",
    "#     loss, acc = model.evaluate(val_ds, verbose=0)\n",
    "#     print(\"âœ… Loaded model from saved weights\")\n",
    "#     print(f\"ğŸ“ˆ Validation Accuracy (from saved model): {acc:.4f}\")\n",
    "#     print(f\"ğŸ“‰ Validation Loss: {loss:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 12025815,
     "sourceId": 100293,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
