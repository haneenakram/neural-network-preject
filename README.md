# Task 2: Multi-Layer Perceptron with Backpropagation

## ðŸ§  Project Overview

This project implements a **Multi-Layer Perceptron (MLP)** neural network using the **Backpropagation learning algorithm** for multi-class classification of bird species. The implementation supports flexible network architecture with customizable hidden layers and activation functions.

## ðŸŽ¯ Algorithm: Backpropagation

### What is Backpropagation?
Backpropagation is a supervised learning algorithm that trains multi-layer neural networks by:
1. **Forward Pass**: Computing predictions through the network
2. **Error Calculation**: Measuring difference between prediction and target
3. **Backward Pass**: Propagating errors back through layers
4. **Weight Updates**: Adjusting weights to minimize error

### Mathematical Foundation
```
Forward Pass:
- z^(l) = W^(l) * a^(l-1) + b^(l)
- a^(l) = Ïƒ(z^(l))

Backward Pass:
- Î´^(L) = âˆ‡_a C âŠ™ Ïƒ'(z^(L))
- Î´^(l) = ((W^(l+1))^T Î´^(l+1)) âŠ™ Ïƒ'(z^(l))

Weight Updates:
- W^(l) = W^(l) - Î· * Î´^(l+1) * (a^(l))^T
- b^(l) = b^(l) - Î· * Î´^(l+1)
```

## ðŸ“Š Dataset: Birds Multi-Class Classification

### Dataset Specifications
- **Total Samples**: 150 birds (50 per species)
- **Species Classes**: A, B, C (3-class classification)
- **Features**: 5 dimensional input vector
  - `gender` (preprocessed to numerical)
  - `body_mass`
  - `beak_length` 
  - `beak_depth`
  - `fin_length`

### Data Split Strategy
- **Training**: First 30 samples from each class (90 total)
- **Testing**: Remaining 20 samples from each class (60 total)
- **Input Dimension**: 5 features
- **Output Dimension**: 3 classes (one-hot encoded)

## ðŸ—ï¸ Network Architecture

### Flexible MLP Design
```
Input Layer (5 neurons)
    â†“
Hidden Layer 1 (n1 neurons) - User defined
    â†“
Hidden Layer 2 (n2 neurons) - Optional
    â†“
...
    â†“
Hidden Layer k (nk neurons) - User defined
    â†“
Output Layer (3 neurons) - Softmax
```

### Activation Functions
1. **Sigmoid**: Ïƒ(x) = 1/(1 + e^(-x))
   - Range: (0, 1)
   - Smooth gradient
   - Can suffer from vanishing gradient

2. **Hyperbolic Tangent**: tanh(x) = (e^x - e^(-x))/(e^x + e^(-x))
   - Range: (-1, 1)
   - Zero-centered
   - Stronger gradients than sigmoid

## ðŸ–¥ï¸ GUI Interface

### User Configuration Options

#### Network Architecture
- **Number of Hidden Layers**: 1, 2, 3, ... (user defined)
- **Neurons per Layer**: Customizable for each hidden layer
- **Bias Option**: Include/exclude bias terms (checkbox)

#### Training Parameters
- **Learning Rate (Î·)**: Controls weight update step size
- **Number of Epochs (m)**: Maximum training iterations
- **Activation Function**: Sigmoid or Hyperbolic Tangent (radio buttons)

#### Fixed Parameters
- **Input Features**: 5 (all bird characteristics)
- **Output Classes**: 3 (species A, B, C)
- **Weight Initialization**: Small random numbers

### GUI Layout
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Network Architecture Configuration  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Hidden Layers: [2] â–¼                â”‚
â”‚ Layer 1 Neurons: [10] â–¼             â”‚
â”‚ Layer 2 Neurons: [8] â–¼              â”‚
â”‚ Learning Rate: [0.01] ___           â”‚
â”‚ Epochs: [1000] ____                 â”‚
â”‚ â˜‘ Use Bias                          â”‚
â”‚ â—‹ Sigmoid â— Tanh                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ [Train Network] [Test Sample]       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Results: Accuracy, Confusion Matrix â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ðŸ“ Project Structure

```
Task2-Backpropagation-MLP/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ MLP.py                 # MLP class implementation
â”‚   â”œâ”€â”€ utils.py               # Data loading and preprocessing
â”‚   â”œâ”€â”€ app.py                 # Main GUI application
â”‚   â””â”€â”€ task2-notebook.py     
â”œâ”€â”€ data/
â”‚   â””â”€â”€ birds.csv              # Bird species dataset
â”œâ”€â”€ report/
â”‚   â””â”€â”€ mlp_report.pdf          
â””â”€â”€ README.md
```

## ðŸš€ Implementation Details

### MLP Class Structure
```python
class MLP:
    def __init__(self, layers, activation='sigmoid', use_bias=True):
        self.layers = layers  # [5, 10, 8, 3] example
        self.activation = activation
        self.use_bias = use_bias
        self.weights = []
        self.biases = []
        
    def forward_pass(self, X):
        # Compute activations layer by layer
        
    def backward_pass(self, X, y):
        # Compute gradients using backpropagation
        
    def update_weights(self, learning_rate):
        # Apply gradient descent updates
        
    def train(self, X, y, epochs, learning_rate):
        # Training loop with error monitoring
```

### Activation Function Implementations
```python
def sigmoid(x):
    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))

def sigmoid_derivative(x):
    s = sigmoid(x)
    return s * (1 - s)

def tanh(x):
    return np.tanh(x)

def tanh_derivative(x):
    return 1 - np.tanh(x)**2
```

## ðŸ“ˆ Performance Analysis

### Experimental Setup
Test various configurations to find optimal parameters:

#### Network Architectures to Test
1. **Single Hidden Layer**: [5, n, 3] where n âˆˆ {5, 10, 15, 20}
2. **Two Hidden Layers**: [5, n1, n2, 3] various combinations
3. **Three Hidden Layers**: [5, n1, n2, n3, 3] selected configurations

#### Hyperparameter Grid Search
- **Learning Rates**: [0.001, 0.01, 0.1, 0.5]
- **Epochs**: [500, 1000, 2000, 5000]
- **Activation Functions**: [Sigmoid, Tanh]
- **Bias**: [With Bias, Without Bias]

## ðŸ” Key Features

### Algorithm Implementation
- âœ… **Full Backpropagation**: Forward and backward pass
- âœ… **Flexible Architecture**: Variable hidden layers and neurons
- âœ… **Multiple Activations**: Sigmoid and Hyperbolic Tangent
- âœ… **Bias Support**: Optional bias terms
- âœ… **Weight Initialization**: Proper random initialization

### Evaluation Metrics
- âœ… **Custom Confusion Matrix**: Implemented from scratch
- âœ… **Accuracy Calculation**: Overall and per-class accuracy
- âœ… **Training Progress**: Loss and accuracy tracking
- âœ… **Convergence Analysis**: Training curve visualization

## ðŸ“Š Deliverables

### Code Requirements
- **Complete MLP Implementation**: All algorithms from scratch
- **Interactive GUI**: User-friendly parameter configuration
- **Comprehensive Testing**: Multiple architecture experiments
- **Performance Analysis**: Detailed comparison of configurations

## ðŸŽ¯ Learning Objectives

This project teaches:
- **Deep Learning Fundamentals**: Multi-layer neural networks
- **Backpropagation Algorithm**: Gradient-based learning
- **Network Design**: Architecture choices and their impact
- **Hyperparameter Tuning**: Systematic parameter optimization
- **Performance Evaluation**: Comprehensive model assessment
- **Software Engineering**: Clean, modular code organization

## âš¡ Quick Start

### Installation
```bash
pip install numpy matplotlib pandas streamlit
```

### Running the Application
```bash
streamlit run src/app.py
```

### Usage Workflow
1. **Configure Network**: Set hidden layers and neurons
2. **Set Parameters**: Choose learning rate, epochs, activation
3. **Train Model**: Click train and monitor progress
4. **Evaluate Results**: Review accuracy and confusion matrix
5. **Experiment**: Try different configurations
6. **Document Best**: Record optimal parameters and performance

## ðŸ† Success Metrics

- âœ… **Functional MLP**: Proper backpropagation implementation
- âœ… **GUI Completeness**: All required features working
- âœ… **High Accuracy**: >90% testing accuracy achieved
- âœ… **Thorough Analysis**: Comprehensive parameter comparison
- âœ… **Clean Code**: Well-structured, documented implementation
- âœ… **Detailed Report**: Professional analysis with screenshots

---

**Goal**: Master the fundamentals of multi-layer neural networks and understand how architectural choices affect learning performance in multi-class classification tasks.