# Fine-Grained Fruit Quality Assessment using Deep Learning

## Team CHP_5

- **Haneen Akram Ahmed**
- **Menna Ali Thabet**
- **Mohamed Ashraf Fathy**
- **Reem Ahmed Ismail**
- **Zeina Shawkat**
- **Islam Hesham**

## ğŸš€ Neural Networks Course - Final Project

This repository showcases an advanced deep learning project for **fine-grained fruit quality assessment**, implementing and comparing **6 different neural network architectures** to classify fruits into specific ripeness and quality categories. This project demonstrates mastery of modern deep learning techniques, from CNNs to Vision Transformers.

> **ğŸ’¡ Course Foundation**: This project builds upon fundamental neural network concepts learned through implementing [Perceptron & Adaline algorithms](../../tree/task1) and [Multi-Layer Perceptron with Backpropagation](../../tree/task2) from scratch.

## ğŸ“‹ Project Overview

### ğŸ¯ Objective

Develop robust deep learning models capable of performing fine-grained classification of fruit quality using image features, specifically targeting:

- **Banana categories**: overripe, ripe, rotten, unripe
- **Tomato categories**: fully ripened, green, half ripened

### ğŸ† Key Achievements

- **96.00% validation accuracy** using CNN + Transformer Hybrid
- **6 different architectures** implemented and compared
- **Severe class imbalance handled** (39.74:1 ratio)
- **7,395 images** processed across 7 quality categories
- **Custom Vision Transformer** designed specifically for fruit textures

## ğŸ“Š Dataset Challenge

The dataset presents a significant **class imbalance challenge**:

| Category             | Count | Percentage |
| -------------------- | ----- | ---------- |
| banana_rotten        | 1,987 | 26.9%      |
| banana_ripe          | 1,440 | 19.5%      |
| banana_overripe      | 1,395 | 18.9%      |
| banana_unripe        | 1,370 | 18.5%      |
| tomato_green         | 334   | 4.5%       |
| tomato_half_ripened  | 81    | 1.1%       |
| tomato_fully_ripened | 50    | 0.7%       |

**Challenge**: 39.74:1 imbalance ratio between largest and smallest classes

## ğŸ—ï¸ Model Architectures & Results

### ğŸ¥‡ Top Performing Models

| Model              | Validation Accuracy | Key Innovation |
| ------------------ | ------------------- | -------------- |
| **CNN + Transformer** | **96.00%** ğŸ† | Hybrid local-global feature fusion |
| **CoAtNet**            | **95.45%** ğŸ¥ˆ | Convolutional + Attention integration |
| **Vision Transformer** | **95.39%** ğŸ¥‰ | Custom ViT for fruit texture analysis |
| ResNet-34             | ~95%        | Auxiliary classifier enhancement |
| Custom ResNet-50      | 93-94%      | Enhanced regularization |
| Baseline CNN          | 92.2%       | Three-block foundation |

### ğŸ§  Architecture Highlights

#### 1. Vision Transformer (Custom Design)
```
Input: 224Ã—224Ã—3 â†’ 784 patches (8Ã—8)
â”œâ”€â”€ Patch Embedding: 128-dim + positional encoding
â”œâ”€â”€ Transformer: 8 layers, 8 heads, GELU activation  
â”œâ”€â”€ Global Average Pooling
â””â”€â”€ MLP Head: 2048â†’1024â†’7 classes
```
- **Parameters**: 7.77M (29.65 MB)
- **Innovation**: Optimized patch size for fruit texture details

#### 2. CNN + Transformer Hybrid ğŸ†
- **Best Performance**: 96.00% validation accuracy
- **Design**: CNN feature extraction + Transformer attention
- **Advantage**: Combines local texture analysis with global reasoning

#### 3. CoAtNet (Convolution + Attention)
- **Architecture**: Conv stem â†’ MBConv blocks â†’ Transformer blocks
- **Strength**: Seamless integration of convolutional and attention mechanisms

## ğŸ”§ Technical Implementation

### Advanced Training Techniques
- **Class Weighting**: Dynamic weights (0.48 to 19.02) for imbalance handling
- **Data Augmentation**: Rotation, zoom, shifts, brightness adjustment
- **Optimization**: AdamW with weight decay (1e-5)
- **Learning Rate**: 1e-4 with reduction on plateau
- **Regularization**: Early stopping, dropout, batch normalization

### Key Technical Skills Demonstrated
- âœ… **Custom Architecture Design**: Built Vision Transformer from scratch
- âœ… **Class Imbalance Mastery**: Effective weighting strategies
- âœ… **Hybrid Model Development**: CNN-Transformer fusion
- âœ… **Performance Optimization**: Systematic hyperparameter tuning
- âœ… **Deep Learning Engineering**: Complete training pipelines

## ğŸ“ Repository Structure

```
fruit-quality-assessment/
â”œâ”€â”€ CNN/                              # Baseline convolutional model
â”‚   â”œâ”€â”€ CNN_Report.pdf
â”‚   â””â”€â”€ cnn.ipynb
â”œâ”€â”€ CNN + Transformer Hybrid Model/   # Best performing model
â”‚   â”œâ”€â”€ CNN+Transformer.ipynb
â”‚   â””â”€â”€ CNN+Transformer_Hybrid_Report.pdf
â”œâ”€â”€ CoAtNet/                          # Convolutional + Attention
â”‚   â”œâ”€â”€ coAtNet.ipynb
â”‚   â””â”€â”€ CoAtNet_Report.pdf
â”œâ”€â”€ ResNet-34/                        # Residual network variant
â”‚   â”œâ”€â”€ ResNet34.ipynb
â”‚   â””â”€â”€ ResNet34_Report.pdf
â”œâ”€â”€ ResNet-50/                        # Custom ResNet implementation
â”‚   â”œâ”€â”€ resnet50.ipynb
â”‚   â””â”€â”€ ResNet50_Report.pdf
â”œâ”€â”€ ViT/                             # Custom Vision Transformer
â”‚   â”œâ”€â”€ ViT.ipynb
â”‚   â”œâ”€â”€ Vision_Transformer_Report.py
â”‚   â”œâ”€â”€ vit_classes.py
â”‚   â”œâ”€â”€ fruit_quality_vit_complete.keras
â”‚   â””â”€â”€ vit_weights.h5
â”œâ”€â”€ class_indeces.json
â””â”€â”€ README.md
```
## ğŸ“ˆ Course Learning Journey

This project represents the culmination of a comprehensive neural networks course:

### ğŸ“ Foundation â†’ Advanced Progression

```
â”œâ”€â”€ Task 1: Perceptron & Adaline (Branch: task1-perceptron-adaline)
â”‚   â”œâ”€â”€ Single-layer networks from scratch
â”‚   â”œâ”€â”€ Binary classification fundamentals  
â”‚   â”œâ”€â”€ Interactive GUI development
â”‚   â””â”€â”€ Custom evaluation metrics
â”‚
â”œâ”€â”€ Task 2: Multi-Layer Perceptron (Branch: task2-backpropagation-mlp)
â”‚   â”œâ”€â”€ Backpropagation algorithm implementation
â”‚   â”œâ”€â”€ Multi-class classification
â”‚   â”œâ”€â”€ Interactive GUI development
â”‚   â”œâ”€â”€ Architecture design principles
â”‚   â””â”€â”€ Hyperparameter optimization
â”‚
â””â”€â”€ Final Project: Advanced Deep Learning (Main Branch)
    â”œâ”€â”€ State-of-the-art architectures
    â”œâ”€â”€ Vision Transformers & Hybrid models
    â”œâ”€â”€ Real-world dataset challenges
    â””â”€â”€ Production-ready performance
```

### ğŸ”— Related Course Work
- **[Task 1: Perceptron & Adaline](../../tree/task1)** - Foundation algorithms
- **[Task 2: Backpropagation MLP](../../tree/task2)** - Core deep learning concepts

## ğŸ¯ Key Learning Outcomes

### **Deep Learning Expertise**
- Designed and implemented Vision Transformers from mathematical foundations
- Built hybrid CNN-Transformer architectures for optimal performance
- Mastered attention mechanisms and their application to computer vision
- Achieved state-of-the-art results on challenging imbalanced datasets

### **ML Engineering Skills**
- Handled severe class imbalance using advanced weighting techniques
- Implemented comprehensive model comparison frameworks
- Applied systematic hyperparameter optimization strategies
- Developed robust training pipelines with proper regularization

### **Research & Analysis**
- Conducted thorough experimental comparisons across 6 architectures
- Performed ablation studies on architectural components
- Generated detailed technical reports with performance analysis
- Demonstrated scientific rigor in model evaluation and selection

## ğŸ† Impact & Results

- **ğŸ¯ Accuracy**: 96% validation accuracy on challenging dataset
- **âš–ï¸ Balance**: Successfully handled 39:1 class imbalance
- **ğŸ§  Innovation**: Custom ViT architecture outperformed standard approaches
- **ğŸ“Š Comparison**: Systematic evaluation of 6 different deep learning architectures
- **ğŸ“ˆ Scalability**: Models ready for production deployment

## ğŸ“š Technical References

1. Dosovitskiy, A., et al. "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale." ICLR 2021.
2. Dai, Z., et al. "CoAtNet: Marrying Convolution and Attention for All Data Sizes." NeurIPS 2021.
3. He, K., et al. "Deep Residual Learning for Image Recognition." CVPR 2016.

---

**ğŸ’¡ This project demonstrates advanced deep learning expertise, from implementing algorithms from scratch to designing state-of-the-art architectures for real-world applications.**

_Neural Networks Course - Spring 2025_